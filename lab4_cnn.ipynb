{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Classification using Machine Learning in HealthCare**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **2** hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is dedicated to application of an advanced analysis of Big Data to the spread of COVID-19 in the world. We will study how to make image recognition of a chest X-ray to provide a diagnosis forecast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X-rays are widely used in medical practice. They can be used to identify various diseases. However, a diagnosis depends on a doctor's experience, which can lead to improper treatment. Modern methods of artificial intelligence and pattern recognition make it possible to create expert systems that allow you to establish a diagnosis automatically.\n",
    "\n",
    "This lab will show you how to upload images, transform them, and determine the basic features that underlie diseases classification.\n",
    "\n",
    "Two different approaches to the classification of images (diseases) will be shown:\n",
    "1. Different classical methods and their comparison \n",
    "2. Convolutional Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn the basic methods of images classifications. The laboratory consists of four stages:\n",
    "* Download and preliminary transformation of images\n",
    "* Creating image features\n",
    "* Comparing different classical classification methods\n",
    "* Building and fitting Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical data was obtained from https://www.kaggle.com/pranavraikokte/covid19-image-dataset under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* [Python](https://www.python.org)\n",
    "* [os](https://docs.python.org/3/library/os.html)\n",
    "* [numpy](https://numpy.org)\n",
    "* [glob](https://docs.python.org/3/library/glob.html)\n",
    "* [SeaBorn](https://seaborn.pydata.org)\n",
    "* [Matplotlib](https://matplotlib.org)\n",
    "* [mahotas](https://mahotas.readthedocs.io/en/latest/)\n",
    "* [keras](https://keras.io)\n",
    "* [scikit-learn](https://scikit-learn.org)\n",
    "* [pandas](https://pandas.pydata.org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download and transform images.\n",
    "* Create features of images.\n",
    "* Build different classification models.\n",
    "* Build CNN models.\n",
    "* Build a diagnosis based on X-ray photos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preliminary transformation of images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install additional libraries and upgrade existing ones in the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install matplotlib >= 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda config --add channels conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mahotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge tensorflow --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge keras --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to use Mahotas for image processing and Keras library for creating our CNN model and its training. We will also use Matplotlib and Seaborn for visualizing our dataset to gain a better understanding of the images we are going to handle.\n",
    "We will also use libraries os and glob to work with files and folders. NumPy will be applied for arrays of images. Scikit-Learn will be used for classical classification models. And we will take Pandas for present comparison of classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mahotas as mh\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "#Classifiers\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skillsnetwork\n",
    "\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-machine-learning-classification/LargeData/Covid19-dataset.zip\", overwrite=True)\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-machine-learning-classification/NN.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images have to be of the same size for classification. In order to achieve this, let's create a global variable that will determine the size (height and width) for image resizing. Both are 224 in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMM_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we create a function that downloads and displays all the pictures from a specified directory.\n",
    "In order to classify images, all pictures should be placed in subdirectories. The names of these subdirectories are actually class names.\n",
    "In our case, the images are X-rays which should be placed into subfolders with the names of diagnoses.\n",
    "For example, a COVID subfolder is to contain X-rays of people with this disease.\n",
    "First of all, we should create a list of subfolders, which is a list of possible disease classes. In our case, it will be: Normal, COVID, Viral Pneumonia.\n",
    "\n",
    "Next, we need to create a list of all images.\n",
    "After that, let's download and preprocess all the images:\n",
    "1. Download by [mahotas.imread()](https://mahotas.readthedocs.io/en/latest/io.html)\n",
    "2. It is necessary to resize the images to (IMM_SIZE x IMM_SIZE). If an image is gray-colored it is presented as a 2D matrix: (high, width). jpg and png images are 3D (high, width, 3 or 4). To do this, we should use: [mahotas.resize_to()](https://github.com/luispedro/mahotas/blob/master/mahotas/resize.py)\n",
    "3. If the third parameter of an image shape equals 4, it means alpha channel. We can remove it using slice image[:,:,:3].\n",
    "4. Then we need to transform all the images to gray 2D format by: [mahotas.colors.rgb2grey()](https://mahotas.readthedocs.io/en/latest/color.html)\n",
    "\n",
    "The function returns an array of tuples [image, class name].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    \n",
    "    class_names = [f for f in os.listdir(folder) if not f.startswith('.')] # ctreate a list of SubFolders\n",
    "    data = []\n",
    "    # class_names = ['Covid', 'Normal', 'Viral Pneumonia']\n",
    "    print(class_names)\n",
    "    for t, f in enumerate(class_names):\n",
    "        images = glob(folder + \"/\" + f + \"/*\") # create a list of files\n",
    "        print(\"Downloading: \", f)\n",
    "        fig = plt.figure(figsize = (50,50)) \n",
    "        for im_n, im in enumerate(images):\n",
    "            plt.gray() # set grey colormap of images\n",
    "            image = mh.imread(im)\n",
    "            if len(image.shape) > 2:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE, image.shape[2]]) # resize of RGB and png images\n",
    "            else:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE]) # resize of grey images    \n",
    "            if len(image.shape) > 2:\n",
    "                image = mh.colors.rgb2grey(image[:,:,:3], dtype = np.uint8)  # change of colormap of images alpha chanel delete\n",
    "            plt.subplot(int(len(images)/5)+1,5,im_n+1) # create a table of images\n",
    "            plt.imshow(image)\n",
    "            data.append([image, f])\n",
    "        plt.show()\n",
    "\n",
    "    return np.array(data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and testing, all the pictures should be divided into training and test groups and located in separate folders. Let's upload all the images to the corresponding arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"Covid19-dataset/train\"\n",
    "train = get_data(d)\n",
    "\n",
    "d = \"Covid19-dataset/test\"\n",
    "val = get_data(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape\", train.shape) # Size of the training DataSet\n",
    "print(\"Test shape\", val.shape) # Size of the test DataSet\n",
    "print(\"Image size\", train[0][0].shape) # Size of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training DataSet consists of 251 images and the test one consists of 66 images. All the images are in grey 2D (224x224) format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s visualize our data and see what exactly we are working with. We use Seaborn to plot the number of images in all the classes. You can see what the output looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in train:\n",
    "    l.append(i[1])\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also visualize the first image from the Viral Pneumonia and Covid classes in the training DataSet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[np.where(train[:,1] == 'Viral Pneumonia')[0][0]][0])\n",
    "plt.title('Viral Pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[np.where(train[:,1] == 'Covid')[0][0]][0])\n",
    "plt.title('Covid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image features creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify objects, you need to transform the data set so that the input is a set of features and the output is an object class. An image is a matrix of pixels. Each pixel is a color. Therefore, it is impossible to submit a bare picture into a classical classifier's input. It is necessary to turn each picture into a set of certain features.\n",
    "\n",
    "Mahotas makes it easy to calculate features of an image.\n",
    "The corresponding functions are found in the [mahotas.features](https://mahotas.readthedocs.io/en/latest/) submodule. The Haralick set of texture features is well known. Like many image processing algorithms, it is named for its inventor. The features are based on textures, that is, they distinguish between structured and unstructured images, as well as various repetitive structures. With the help of Mahotas, these features are calculated very easily:\n",
    "haralick_features = mh.features.haralick (image)\n",
    "haralick_features_mean = np.mean (haralick_features, axis = O)\n",
    "haralick_features_all = np.ravel (haralick_features)\n",
    "The mh.features.haralick function returns a 4 x 13 array wiсh should be transformad into 1D by [NumPy.ravel()](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html). The first dimension is the four possible directions in which the features are calculated (vertical, horizontal, and two diagonals). If we are not interested in any particular direction, then we can average the features in all directions (in the code above, this variable is called haralick_features_mean). Alternatively, you can use all the characteristics individually (variable haralick_features_all). The choice depends on the properties of a particular dataset. We decided that the vertical and horizontal features should be stored separately in our case, so we use haralick_features_all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should make a function for the DataSet features creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for image, label in data:\n",
    "        features.append(mh.features.haralick(image).ravel())\n",
    "        labels.append(label)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return (features, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, labels_train = create_features(train)\n",
    "features_test, labels_test = create_features(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different classical classification methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compare some classificators, we should use a Pipeline.\n",
    "A pipeline helps to chain multiple estimators into a single one. This is useful as there is often a fixed number of steps in data processing, for example, feature selection, normalization and classification. A pipeline serves multiple purposes here:\n",
    "\n",
    "You only have to call fit() once to evaluate a whole sequence of estimators.\n",
    "\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Pipelines help to avoid leaking statistics from your test data into the trained model in cross-validation by ensuring that the same samples are used to train the transformers and predictors.\n",
    "All estimators in a pipeline, except the last one, should be transformers (i.e. should have a transform method). The last estimator may be of any type (transformer, classifier, etc.).\n",
    "\n",
    "The [sklearn.pipeline](https://scikit-learn.org/stable/modules/classes.html?highlight=pipeline#module-sklearn.pipeline) module implements utilities to build a composite estimator, as a chain of transforms and estimators.\n",
    "\n",
    "In order to test how it works, we will use LogisticRegression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('preproc', StandardScaler()), ('classifier', LogisticRegression())])\n",
    "clf.fit(features_train, labels_train)\n",
    "scores_train = clf.score(features_train, labels_train)\n",
    "scores_test = clf.score(features_test, labels_test)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))\n",
    "plot_confusion_matrix(clf, features_test, labels_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are not bad.\n",
    "Conflusion matrix shows us how many mistaken predictions we got.\n",
    "It allows us to check other classifiers and compare the results.\n",
    "We will test:\n",
    "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n",
    "* [Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html?highlight=nearest%20neighbors#sklearn.neighbors.NearestNeighbors)\n",
    "* [Linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html?highlight=linear%20svm#sklearn.svm.LinearSVR)\n",
    "* [RBF SVM](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html?highlight=rbf#sklearn.gaussian_process.kernels.RBF)\n",
    "* [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html?highlight=gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier)\n",
    "* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n",
    "* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
    "* [Multi-layer Perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlpclassifier#sklearn.neural_network.MLPClassifier)\n",
    "* [Ada Boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n",
    "* [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB)\n",
    "* [Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html?highlight=quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(\"Fitting:\", name)\n",
    "    clf = Pipeline([('preproc', StandardScaler()), ('classifier', clf)])\n",
    "    clf.fit(features_train, labels_train)\n",
    "    score_train = clf.score(features_train, labels_train)\n",
    "    score_test = clf.score(features_test, labels_test)\n",
    "    scores_train.append(score_train)\n",
    "    scores_test.append(score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the results as a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index = names)\n",
    "res['scores_train'] = scores_train\n",
    "res['scores_test'] = scores_test\n",
    "res.columns = ['Test','Train']\n",
    "res.index.name = \"Classifier accuracy\"\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the calculations are very fast and that Logistic Regression and Neural Network show the best result for the test DataSet.\n",
    "\n",
    "Let's compare the results on a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, scores_train, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, scores_test, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of classifiers')\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_xticklabels(names)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and fitting Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras library for creating and training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is different about [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) is that we can submit images directly to the input. However, these images also require pre-processing. \n",
    "\n",
    "In particular, it is necessary to normalize the pixels color, i.e. to normalize them from the range [0, 255) to [0, 1).\n",
    "\n",
    "You also need to change the dimension of the input images because of Keras framework. \n",
    "\n",
    "Image classes should be of numeric type instead of string.\n",
    "\n",
    "The code below makes the necessary transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "# Reshaping input images\n",
    "x_train = x_train.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "x_val = x_val.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "\n",
    "# Creating a dictionary of clases\n",
    "lab = {}\n",
    "for i, l in enumerate(set(y_train)):\n",
    "    lab[l] = i\n",
    "\n",
    "\n",
    "y_train = np.array([lab[l] for l in y_train])\n",
    "y_val = np.array([lab[l] for l in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the input DataSet:\", x_train.shape)\n",
    "print(\"Shape of the output DataSet:\", y_train.shape)\n",
    "print(\"Dictionary of classes:\", lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation on the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should perform data augmentation to fit our model better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model defining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define a simple CNN model with 3 Convolutional layers followed by max-pooling layers. A dropout layer is added after the 3rd maxpool operation to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,1,padding=\"same\", activation=\"relu\", input_shape=(IMM_SIZE,IMM_SIZE,1)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compile the model now using Adam as our optimizer and SparseCategoricalCrossentropy as the loss function. We are using a lower learning rate of 0.000001 for a smoother curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train our model for **2000** epochs.\n",
    "Admittedly, the fitting process is very slow. Therefore, we saved the fitted model to a file.\n",
    "To save time, we will upload the fitted model.\n",
    "If you would like, you can change the parameter **fitting to True** in order to refit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = False\n",
    "fitting_save = False\n",
    "epochs = 200\n",
    "\n",
    "import pickle\n",
    "\n",
    "if fitting:\n",
    "    history = model.fit(x_train,y_train,epochs = epochs , validation_data = (x_val, y_val), shuffle = True)\n",
    "    if fitting_save:\n",
    "    # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        with open('history.pickle', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        with open('lab.pickle', 'wb') as f:\n",
    "            pickle.dump(lab, f)\n",
    "# load model  \n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into a new model\n",
    "model.load_weights(\"model.h5\")        \n",
    "with open('history.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot our training and validation accuracy along with the training and validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what the curve looks like.\n",
    "You can see that the accuracy of the training and validation sets is the same. The loss function of validation and training sets is stable. It means that our CNN is fitted well and can be used for classification.\n",
    "\n",
    "We can print out the classification report to see the precision and accuracy using [model.predict_classes()]() and [classification_report()]().\n",
    "\n",
    "Also we can create a confusion matrix. Unfortunately, Keras framework does not have plot_confusion_matrix() function. Therefore, we have to create it using Pandas and [Seaborn.heatmap()]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "predictions = model.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_val, predictions, target_names = lab.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, predictions))\n",
    "cm.index = [\"Predicted \" + s for s in lab.keys()]\n",
    "cm.columns = [\"True  \" + s for s in lab.keys()]\n",
    "print(cm)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_val, predictions), annot=True, \n",
    "            xticklabels = list(lab.keys()), yticklabels = list(lab.keys()))\n",
    "plt.xlabel(\"True labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "z = model.predict_classes(x_train) == y_train\n",
    "scores_train = sum(z+0)/len(z)\n",
    "z = model.predict_classes(x_val) == y_val\n",
    "scores_test = sum(z+0)/len(z)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CNN shows better results than classical models. However, fitting takes much longer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, on your own, try to create a function that will establish a diagnosis based on the CNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosis(file):\n",
    "    # Download image\n",
    "    ##YOUR CODE GOES HERE##\n",
    "    \n",
    "    # Prepare image to classification\n",
    "    ##YOUR CODE GOES HERE##\n",
    "        \n",
    "    # Show image\n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "\n",
    "    # Load model  \n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "\n",
    "    # Normalize the data\n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "    \n",
    "    # Reshape input images\n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "    \n",
    "    # Predict the diagnosis\n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "    \n",
    "    # Find the name of the diagnosis  \n",
    "    ##YOUR CODE GOES HERE##\n",
    "\n",
    "    \n",
    "    return diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Download image</b></summary> \n",
    "    <code>\n",
    "    try:\n",
    "        image = mh.imread(file)\n",
    "    except:\n",
    "        print(\"Cannot download image: \", file)\n",
    "        return\n",
    "        </code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Prepare image to classification</b></summary> \n",
    "    <code>\n",
    "    if len(image.shape) > 2:\n",
    "        image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE, image.shape[2]]) #resize of images RGB and png\n",
    "    else:\n",
    "        image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE]) #resize of grey images    \n",
    "    if len(image.shape) > 2:\n",
    "        image = mh.colors.rgb2grey(image[:,:,:3], dtype = np.uint8)  #change of colormap of images alpha chanel delete\n",
    "     </code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Show image</b></summary> \n",
    "    <code>\n",
    "    plt.gray()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    </code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Load model</b></summary> \n",
    "    <code>\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"model.h5\")        \n",
    "    with open('history.pickle', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    with open('lab.pickle', 'rb') as f:\n",
    "        lab = pickle.load(f)\n",
    "   </code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Normalize the data</b></summary> \n",
    "<code>\n",
    "image = np.array(image) / 255\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Reshaping input images</b></summary> \n",
    "<code>\n",
    "    image = image.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Predict diagnosis</b></summary> \n",
    "<code>\n",
    "diag = model.predict_classes(image)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution for <b>Find name of diagnosis </b></summary> \n",
    "<code>\n",
    "diag =list(lab.keys())[list(lab.values()).index(diag[0])]\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Diagnosis is:\", diagnosis(\"Covid19-dataset/test/Covid/0120.jpg\"))\n",
    "print (\"Diagnosis is:\", diagnosis(\"Covid19-dataset/test/Normal/0105.jpeg\"))\n",
    "print (\"Diagnosis is:\", diagnosis(\"Covid19-dataset/test/Viral Pneumonia/0111.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab has revealed for us how to create an expert system that allows us to obtain diagnosis based on X-Rays images, using different classificators. This principles can be used for any type of X-Rays, not only for COVID diagnostics.\n",
    "\n",
    "During this laboratory work, we have explored images downloading and transforming. We have learned how to extract features of images and build/fit/test/compare sets of classificators. Also we got to know how to create and fit Convolutional Neural Networks. We have compared accuracy of different classificators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yaroslav Vyklyuk, prof., PhD., DrSc](https://author.skills.network/instructors/yaroslav_vyklyuk_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copyright &copy; 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
